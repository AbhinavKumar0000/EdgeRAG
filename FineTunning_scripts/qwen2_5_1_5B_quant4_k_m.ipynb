{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kB6iDzeTG5ZB",
        "outputId": "1616636d-002b-4499-b1ba-eca51f7aac8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 29 03:39:04 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,225 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,572 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,966 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,633 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n",
            "Get:24 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,860 kB]\n",
            "Fetched 38.3 MB in 5s (7,747 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "The following additional packages will be installed:\n",
            "  javascript-common libjs-sphinxdoc libjs-underscore python3-dev\n",
            "  python3-pkg-resources python3-setuptools python3-wheel python3.10-dev\n",
            "Suggested packages:\n",
            "  apache2 | lighttpd | httpd python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  javascript-common libjs-sphinxdoc libjs-underscore python3-dev python3-pip\n",
            "  python3-setuptools python3-wheel python3.10-dev\n",
            "The following packages will be upgraded:\n",
            "  python3-pkg-resources\n",
            "1 upgraded, 8 newly installed, 0 to remove and 53 not upgraded.\n",
            "Need to get 2,816 kB of archives.\n",
            "After this operation, 10.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 javascript-common all 11+nmu1 [5,936 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-underscore all 1.13.2~dfsg-2 [118 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pkg-resources all 68.1.2-2~jammy3 [216 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-sphinxdoc all 4.3.2-1 [139 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3.10-dev amd64 3.10.12-1~22.04.12 [508 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-dev amd64 3.10.6-1~22.04.1 [26.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.7 [1,306 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-setuptools all 68.1.2-2~jammy3 [465 kB]\n",
            "Fetched 2,816 kB in 2s (1,150 kB/s)\n",
            "Selecting previously unselected package javascript-common.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../0-javascript-common_11+nmu1_all.deb ...\n",
            "Unpacking javascript-common (11+nmu1) ...\n",
            "Selecting previously unselected package libjs-underscore.\n",
            "Preparing to unpack .../1-libjs-underscore_1.13.2~dfsg-2_all.deb ...\n",
            "Unpacking libjs-underscore (1.13.2~dfsg-2) ...\n",
            "Selecting previously unselected package libjs-sphinxdoc.\n",
            "Preparing to unpack .../2-libjs-sphinxdoc_4.3.2-1_all.deb ...\n",
            "Unpacking libjs-sphinxdoc (4.3.2-1) ...\n",
            "Selecting previously unselected package python3.10-dev.\n",
            "Preparing to unpack .../3-python3.10-dev_3.10.12-1~22.04.12_amd64.deb ...\n",
            "Unpacking python3.10-dev (3.10.12-1~22.04.12) ...\n",
            "Selecting previously unselected package python3-dev.\n",
            "Preparing to unpack .../4-python3-dev_3.10.6-1~22.04.1_amd64.deb ...\n",
            "Unpacking python3-dev (3.10.6-1~22.04.1) ...\n",
            "Preparing to unpack .../5-python3-pkg-resources_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-pkg-resources (68.1.2-2~jammy3) over (59.6.0-1.2ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../6-python3-setuptools_68.1.2-2~jammy3_all.deb ...\n",
            "Unpacking python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../7-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../8-python3-pip_22.0.2+dfsg-1ubuntu0.7_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\n",
            "Setting up python3-pkg-resources (68.1.2-2~jammy3) ...\n",
            "Setting up javascript-common (11+nmu1) ...\n",
            "Setting up python3-setuptools (68.1.2-2~jammy3) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.7) ...\n",
            "Setting up python3.10-dev (3.10.12-1~22.04.12) ...\n",
            "Setting up libjs-underscore (1.13.2~dfsg-2) ...\n",
            "Setting up libjs-sphinxdoc (4.3.2-1) ...\n",
            "Setting up python3-dev (3.10.6-1~22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 73538, done.\u001b[K\n",
            "remote: Total 73538 (delta 0), reused 0 (delta 0), pack-reused 73538 (from 1)\u001b[K\n",
            "Receiving objects: 100% (73538/73538), 250.88 MiB | 14.81 MiB/s, done.\n",
            "Resolving deltas: 100% (53273/53273), done.\n",
            "/content/llama.cpp\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Collecting numpy~=1.26.4 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.57.1 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.57.3)\n",
            "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 6))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert_legacy_llama.txt (line 7))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting torch~=2.6.0 (from -r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp312-cp312-linux_x86_64.whl.metadata (26 kB)\n",
            "Collecting aiohttp~=3.9.3 (from -r ./requirements/requirements-tool_bench.txt (line 1))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/aiohttp-3.9.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest~=8.3.3 (from -r ./requirements/requirements-tool_bench.txt (line 2))\n",
            "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 3)) (0.36.0)\n",
            "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 4)) (3.10.0)\n",
            "Collecting openai~=1.55.3 (from -r ./requirements/requirements-tool_bench.txt (line 6))\n",
            "  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pandas~=2.2.3 (from -r ./requirements/requirements-tool_bench.txt (line 7))\n",
            "  Downloading https://download.pytorch.org/whl/nightly/pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client~=0.20.0 (from -r ./requirements/requirements-tool_bench.txt (line 8))\n",
            "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 9)) (2.32.4)\n",
            "Collecting wget~=3.2 (from -r ./requirements/requirements-tool_bench.txt (line 10))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typer~=0.15.1 (from -r ./requirements/requirements-tool_bench.txt (line 11))\n",
            "  Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (3.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5))\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (6.7.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.22.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.12/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0,>=0.34.0->-r ./requirements/requirements-tool_bench.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2025.11.12)\n",
            "Collecting click<8.2,>=8.0.0 (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=1.55.3->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/nightly/gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp312-cp312-linux_x86_64.whl (178.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.6/178.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=693b7abd1c0e1c20b815069cf97f25f062d53219fec71f294658be01f41bbe1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, sympy, pytest, protobuf, prometheus-client, numpy, click, torch, pandas, gguf, aiohttp, typer, openai\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.4.2\n",
            "    Uninstalling pytest-8.4.2:\n",
            "      Successfully uninstalled pytest-8.4.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus_client 0.23.1\n",
            "    Uninstalling prometheus_client-0.23.1:\n",
            "      Successfully uninstalled prometheus_client-0.23.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.13.2\n",
            "    Uninstalling aiohttp-3.13.2:\n",
            "      Successfully uninstalled aiohttp-3.13.2\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.20.0\n",
            "    Uninstalling typer-0.20.0:\n",
            "      Successfully uninstalled typer-0.20.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.12.0\n",
            "    Uninstalling openai-2.12.0:\n",
            "      Successfully uninstalled openai-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.6.0+cpu which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.6.0+cpu which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.5 click-8.1.8 gguf-0.17.1 numpy-1.26.4 openai-1.55.3 pandas-2.2.3 prometheus-client-0.20.0 protobuf-4.25.8 pytest-8.3.5 sympy-1.13.1 torch-2.6.0+cpu typer-0.15.4 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "a6fd4657d2d34fd9b29130e60758fa6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "-- CUDA Toolkit found\n",
            "-- Using CUDA architectures: native\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- CUDA host compiler is GNU 11.4.0\n",
            "-- Including CUDA backend\n",
            "-- ggml version: 0.9.4\n",
            "-- ggml commit:  382808c14\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (9.4s)\n",
            "-- Generating done (0.4s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  0%] Built target build_info\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  3%] Built target ggml-base\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  4%] Built target sha256\n",
            "[  5%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  5%] Built target xxhash\n",
            "[  5%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  5%] Built target sha1\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  6%] Built target llama-llava-cli\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  6%] Built target llama-gemma3-cli\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  6%] Built target llama-minicpmv-cli\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  6%] Built target llama-qwen2vl-cli\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
            "[  6%] Built target cpp-httplib\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CUDA shared library ../../../bin/libggml-cuda.so\u001b[0m\n",
            "[ 33%] Built target ggml-cuda\n",
            "[ 33%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 33%] Built target ggml\n",
            "[ 33%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 33%] Built target llama-gguf-hash\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 35%] Built target llama-gguf\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 58%] Built target llama\n",
            "[ 58%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 61%] Built target mtmd\n",
            "[ 61%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 62%] Built target test-c\n",
            "[ 63%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 64%] Built target llama-simple\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 64%] Built target llama-simple-chat\n",
            "[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 66%] Built target common\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 68%] Built target test-tokenizer-0\n",
            "[ 68%] Built target test-sampling\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 68%] Built target test-grammar-parser\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 69%] Built target test-llama-grammar\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 70%] Built target test-grammar-integration\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 70%] Built target test-json-schema-to-grammar\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 70%] Built target test-quantize-stats\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 71%] Built target test-gbnf-validator\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 71%] Built target test-tokenizer-1-bpe\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 71%] Built target test-tokenizer-1-spm\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 72%] Built target test-chat-parser\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 72%] Built target test-chat\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 72%] Built target test-chat-template\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 73%] Built target test-json-partial\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 73%] Built target test-log\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n",
            "[ 74%] Built target test-chat-peg-parser\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 76%] Built target test-regex-partial\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 76%] Built target test-thread-safety\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 77%] Built target test-arg-parser\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 77%] Built target test-opt\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 77%] Built target test-gguf\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n",
            "[ 78%] Built target test-peg-parser\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 79%] Built target test-model-load-cancel\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 80%] Built target test-autorelease\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n",
            "[ 81%] Built target test-state-restore-fragmented\n",
            "[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 81%] Built target test-barrier\n",
            "[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 81%] Built target test-quantize-fns\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 82%] Built target test-quantize-perf\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 82%] Built target test-rope\n",
            "[ 82%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 83%] Built target test-mtmd-c-api\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 83%] Built target test-alloc\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 83%] Built target llama-batched\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 83%] Built target llama-embedding\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 84%] Built target llama-eval-callback\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n",
            "[ 84%] Built target llama-idle\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 84%] Built target llama-lookahead\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 85%] Built target llama-lookup\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 85%] Built target test-backend-ops\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 85%] Built target llama-lookup-create\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 86%] Built target llama-lookup-merge\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 86%] Built target llama-lookup-stats\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 86%] Built target llama-parallel\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 86%] Built target llama-passkey\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 86%] Built target llama-retrieval\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 86%] Built target llama-save-load-state\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 87%] Built target llama-speculative-simple\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 87%] Built target llama-speculative\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 87%] Built target llama-gen-docs\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 88%] Built target llama-finetune\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 88%] Built target llama-logits\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 88%] Built target llama-diffusion-cli\n",
            "[ 89%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 89%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 89%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 89%] Built target llama-vdot\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 89%] Built target llama-q8dot\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 91%] Built target llama-gguf-split\n",
            "[ 91%] Built target llama-batched-bench\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 91%] Built target llama-imatrix\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 91%] Built target llama-bench\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n",
            "[ 93%] Built target llama-completion\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 94%] Built target llama-perplexity\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 95%] Built target llama-quantize\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 96%] Built target llama-run\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 96%] Built target llama-tokenize\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n",
            "[ 96%] Built target server-context\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 97%] Built target llama-mtmd-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 97%] Built target llama-cvector-generator\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 97%] Built target llama-tts\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n",
            "[ 98%] Built target llama-fit-params\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 98%] Built target llama-export-lora\n",
            "[ 99%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 99%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 99%] Built target llama-cli\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-task.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-queue.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-common.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-context.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU and install build dependencies\n",
        "!nvidia-smi\n",
        "!apt-get update && apt-get install -y cmake build-essential python3-pip\n",
        "\n",
        "# Clone llama.cpp repository\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "\n",
        "# Install Python requirements for the conversion script\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Build llama.cpp with CUDA (cuBLAS) support for GPU acceleration\n",
        "!cmake -B build -DGGML_CUDA=ON\n",
        "!cmake --build build --config Release -j $(nproc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/Update Hugging Face Hub (Ensure we have the latest 'hf' command)\n",
        "!pip install -U huggingface_hub\n",
        "\n",
        "import os\n",
        "\n",
        "# Define model ID and directory\n",
        "MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "MODEL_DIR = \"model_raw\"\n",
        "\n",
        "# Method 1: Try the modern 'hf' CLI (Flag '--local-dir-use-symlinks' is REMOVED in v1.0+)\n",
        "print(\"Attempting download with 'hf' CLI...\")\n",
        "exit_code = os.system(f\"hf download {MODEL_ID} --local-dir {MODEL_DIR}\")\n",
        "\n",
        "# Method 2: Python Fallback (If CLI fails or isn't in PATH)\n",
        "if exit_code != 0:\n",
        "    print(\"\\nCLI failed or not found. Switching to Python API...\")\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "    # In newer versions, 'local_dir_use_symlinks' is often implicit or deprecated,\n",
        "    # but 'local_dir' ensures files are placed where we want them.\n",
        "    snapshot_download(\n",
        "        repo_id=MODEL_ID,\n",
        "        local_dir=MODEL_DIR,\n",
        "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\"], # Ignore non-GGUF/SafeTensors formats to save space\n",
        "    )\n",
        "\n",
        "print(f\"\\nSuccess! Model downloaded to: {MODEL_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfJCVStXMCnL",
        "outputId": "684f2948-292e-493d-b4b5-052036a616ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub) (8.1.8)\n",
            "Attempting download with 'hf' CLI...\n",
            "\n",
            "Success! Model downloaded to: model_raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Version Conflict: Upgrade transformers to support Qwen 2.5 and newer huggingface-hub\n",
        "!pip install -U transformers accelerate\n",
        "\n",
        "import os\n",
        "\n",
        "# Define paths (Ensure these match your previous cells)\n",
        "MODEL_DIR = \"model_raw\"\n",
        "fp16_output = \"qwen2.5-1.5b-instruct-fp16.gguf\"\n",
        "\n",
        "# Verify model directory exists before running\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    print(f\"Error: Model directory '{MODEL_DIR}' not found. Did Cell 2 finish successfully?\")\n",
        "else:\n",
        "    # Run conversion script\n",
        "    # Usage: convert_hf_to_gguf.py [dir] --outfile [out_file]\n",
        "    !python convert_hf_to_gguf.py {MODEL_DIR} --outfile {fp16_output} --outtype f16\n",
        "\n",
        "    print(f\"Conversion complete: {fp16_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-qjF98ZMOiX",
        "outputId": "fb5fbd0c-842a-4b9d-ff39-1cc853d08f81"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.6.0+cpu)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.2.3\n",
            "    Uninstalling huggingface_hub-1.2.3:\n",
            "      Successfully uninstalled huggingface_hub-1.2.3\n",
            "Successfully installed huggingface-hub-0.36.0\n",
            "INFO:hf-to-gguf:Loading model: model_raw\n",
            "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {1536, 151936}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8960, 1536}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {1536, 8960}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> F16, shape = {1536, 1536}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> F16, shape = {1536, 256}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 1536\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8960\n",
            "INFO:hf-to-gguf:gguf: head count = 12\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151645\n",
            "INFO:gguf.vocab:Setting special token type pad to 151643\n",
            "INFO:gguf.vocab:Setting special token type bos to 151643\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:qwen2.5-1.5b-instruct-fp16.gguf: n_tensors = 338, total_size = 3.1G\n",
            "Writing: 100% 3.09G/3.09G [00:23<00:00, 129Mbyte/s] \n",
            "INFO:hf-to-gguf:Model successfully exported to qwen2.5-1.5b-instruct-fp16.gguf\n",
            "Conversion complete: qwen2.5-1.5b-instruct-fp16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define quantization target\n",
        "quantized_output = \"qwen2.5-1.5b-instruct-q4_k_m.gguf\"\n",
        "quant_method = \"q4_k_m\"\n",
        "binary_path = \"./build/bin/llama-quantize\"\n",
        "\n",
        "# Run quantization\n",
        "# Usage: ./llama-quantize [input_gguf] [output_gguf] [method]\n",
        "!{binary_path} {fp16_output} {quantized_output} {quant_method}\n",
        "\n",
        "print(f\"Quantization complete. File ready: {quantized_output}\")\n",
        "\n",
        "# Cleanup the FP16 file to free up disk space before downloading\n",
        "if os.path.exists(fp16_output):\n",
        "    os.remove(fp16_output)\n",
        "    print(\"Intermediate FP16 file removed to save space.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xwD58iBMO-f",
        "outputId": "93fd0d18-f120-4668-c5b5-dcdebf065627"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 7565 (382808c14)\n",
            "main: built with GNU 11.4.0 for Linux x86_64\n",
            "main: quantizing 'qwen2.5-1.5b-instruct-fp16.gguf' to 'qwen2.5-1.5b-instruct-q4_k_m.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 338 tensors from qwen2.5-1.5b-instruct-fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                     general.sampling.top_k i32              = 20\n",
            "llama_model_loader: - kv   3:                     general.sampling.top_p f32              = 0.800000\n",
            "llama_model_loader: - kv   4:                      general.sampling.temp f32              = 0.700000\n",
            "llama_model_loader: - kv   5:                               general.name str              = Model_Raw\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-1...\n",
            "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv  10:                  general.base_model.0.name str              = Qwen2.5 1.5B\n",
            "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Qwen\n",
            "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-1.5B\n",
            "llama_model_loader: - kv  13:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
            "llama_model_loader: - kv  14:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  15:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv  16:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  17:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  18:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  19:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  20:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  21:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  22:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  23:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "[   1/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[   2/ 338]                    token_embd.weight - [ 1536, 151936,     1,     1], type =    f16, converting to q6_K .. size =   445.12 MiB ->   182.57 MiB\n",
            "[   3/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[   4/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[   5/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[   6/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[   7/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[   8/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[   9/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  10/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  11/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  12/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  13/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  14/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  15/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  16/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  17/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  18/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  19/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  20/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  21/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  22/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  23/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  24/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  25/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  26/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  27/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  28/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  29/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  30/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  31/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  32/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  33/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  34/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  35/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  36/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  37/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  38/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  39/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  40/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  41/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  42/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  43/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  44/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  45/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  46/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  47/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  48/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  49/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  50/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  51/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  52/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  53/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  54/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  55/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  56/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  57/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  58/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  59/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  60/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  61/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  62/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  63/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  64/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  65/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  66/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  67/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  68/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  69/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  70/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[  71/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[  72/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  73/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  74/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  75/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  76/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  77/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  78/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  79/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  80/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  81/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  82/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  83/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  84/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  85/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  86/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  87/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  88/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  89/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  90/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  91/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  92/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[  93/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[  94/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[  95/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  96/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  97/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[  98/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[  99/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 100/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 101/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 102/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 103/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 104/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 105/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 106/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 107/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 108/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 109/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 110/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 111/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 112/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 113/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 114/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 115/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 116/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 117/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 118/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 119/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 120/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 121/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 122/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 123/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 124/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 125/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 126/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 127/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 128/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 129/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 130/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 131/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 132/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 133/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 134/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 135/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 136/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 137/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 138/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 139/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 140/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 141/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 142/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 143/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 144/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 145/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 146/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 147/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 148/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 149/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 150/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 151/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 152/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 153/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 154/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 155/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 156/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 157/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 158/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 159/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 160/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 161/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 162/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 163/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 164/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 165/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 166/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 167/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 168/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 169/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 170/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 171/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 172/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 173/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 174/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 175/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 176/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 177/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 178/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 179/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 180/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 181/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 182/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 183/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 184/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 185/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 186/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 187/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 188/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 189/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 190/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 191/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 192/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 193/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 194/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 195/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 196/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 197/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 198/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 199/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 200/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 201/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 202/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 203/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 204/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 205/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 206/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 207/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 208/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 209/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 210/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 211/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 212/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 213/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 214/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 215/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 216/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 217/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 218/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 219/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 220/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 221/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 222/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 223/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 224/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 225/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 226/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 227/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 228/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 229/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 230/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 231/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 232/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 233/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 234/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 235/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 236/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 237/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 238/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 239/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 240/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 241/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 242/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 243/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 244/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 245/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 246/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 247/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 248/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 249/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 250/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 251/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 252/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 253/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 254/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 255/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 256/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 257/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 258/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 259/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 260/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 261/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 262/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 263/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 264/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 265/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 266/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 267/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 268/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 269/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 270/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 271/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 272/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 273/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 274/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 275/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 276/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 277/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 278/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 279/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 280/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 281/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 282/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 283/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 284/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 285/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 286/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 287/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 288/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 289/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 290/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 291/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 292/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 293/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 294/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 295/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 296/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 297/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 298/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 299/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 300/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 301/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 302/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 303/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 304/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 305/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 306/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 307/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 308/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 309/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 310/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 311/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 312/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 313/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 314/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 315/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 316/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 317/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 318/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 319/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 320/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 321/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 322/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 323/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 324/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 325/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 326/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 327/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 328/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q4_K .. size =     0.75 MiB ->     0.21 MiB\n",
            "[ 329/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 330/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 331/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 332/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q4_K .. size =     4.50 MiB ->     1.27 MiB\n",
            "[ 333/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MiB\n",
            "[ 334/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q6_K .. size =     0.75 MiB ->     0.31 MiB\n",
            "[ 335/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q6_K .. size =    26.25 MiB ->    10.77 MiB\n",
            "[ 336/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "[ 337/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MiB\n",
            "[ 338/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q4_K .. size =    26.25 MiB ->     7.38 MiB\n",
            "llama_model_quantize_impl: model size  =  2944.68 MiB\n",
            "llama_model_quantize_impl: quant size  =   934.69 MiB\n",
            "\n",
            "main: quantize time = 156602.67 ms\n",
            "main:    total time = 156602.67 ms\n",
            "Quantization complete. File ready: qwen2.5-1.5b-instruct-q4_k_m.gguf\n",
            "Intermediate FP16 file removed to save space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick inference test using llama-cli\n",
        "cli_path = \"./build/bin/llama-cli\"\n",
        "prompt = \"Explain quantum computing in one sentence.\"\n",
        "\n",
        "!{cli_path} -m {quantized_output} -p \"{prompt}\" -n 32 -t 4 -ngl 99"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIG01iYNMR-U",
        "outputId": "b0fa9768-f3ac-480d-8c5a-4d813cb9d62d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b \b\n",
            "\n",
            "\n",
            "▄▄ ▄▄\n",
            "██ ██\n",
            "██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄\n",
            "██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██\n",
            "██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀\n",
            "                                    ██    ██\n",
            "                                    ▀▀    ▀▀\n",
            "\n",
            "build      : b7565-382808c14\n",
            "model      : qwen2.5-1.5b-instruct-q4_k_m.gguf\n",
            "modalities : text\n",
            "\n",
            "available commands:\n",
            "  /exit or Ctrl+C     stop or exit\n",
            "  /regen              regenerate the last response\n",
            "  /clear              clear the chat history\n",
            "  /read               add a text file\n",
            "\n",
            "\u001b[1m\u001b[32m\n",
            "> Explain quantum computing in one sentence.\n",
            "\u001b[0m\n",
            "|\b \bQuantum computing leverages quantum-mechanical phenomena to perform operations on data, potentially solving complex problems much faster than classical computers.\n",
            "\u001b[35m\n",
            "[ Prompt: 839.7 t/s | Generation: 88.9 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> hi\n",
            "\u001b[0m\n",
            "|\b \bHello! How can I assist you today?\n",
            "\u001b[35m\n",
            "[ Prompt: 2459.2 t/s | Generation: 84.1 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> hi\n",
            "\u001b[0m\n",
            "|\b \bHello again! How can I help you today?\n",
            "\u001b[35m\n",
            "[ Prompt: 2678.4 t/s | Generation: 84.1 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> what is ai\n",
            "\u001b[0m\n",
            "|\b \bAI, or Artificial Intelligence, is the development of computer systems that can perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making\n",
            "\u001b[35m\n",
            "[ Prompt: 2122.8 t/s | Generation: 87.8 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> /\b/\b/quit\n",
            "\u001b[0m\n",
            "|\b \bI'm sorry, I'm not sure what you mean by \"/quit\". Could you please provide more context or information?\n",
            "\u001b[35m\n",
            "[ Prompt: 3030.3 t/s | Generation: 93.5 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> /\b/\b/exit\n",
            "\u001b[0m\n",
            "\n",
            "Exiting...\n",
            "llama_memory_breakdown_print: | memory breakdown [MiB] | total    free    self   model   context   compute    unaccounted |\n",
            "llama_memory_breakdown_print: |   - CUDA0 (T4)         | 15095 = 12782 + (2185 =   934 +     896 +     355) +         126 |\n",
            "llama_memory_breakdown_print: |   - Host               |                   249 =   182 +       0 +      67                |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define destination in Drive (root folder)\n",
        "destination_path = \"/content/drive/My Drive/\" + quantized_output\n",
        "\n",
        "print(f\"Copying {quantized_output} to Google Drive...\")\n",
        "shutil.copy(quantized_output, destination_path)\n",
        "print(f\"Successfully saved to: {destination_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkRTFKb8MUJ0",
        "outputId": "be7a54f5-d7c2-4ee8-b156-5e7e4e85c8de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copying qwen2.5-1.5b-instruct-q4_k_m.gguf to Google Drive...\n",
            "Successfully saved to: /content/drive/My Drive/qwen2.5-1.5b-instruct-q4_k_m.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTiYBBoCMZBH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}